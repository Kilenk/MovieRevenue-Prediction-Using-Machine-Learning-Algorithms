# -*- coding: utf-8 -*-
"""Movie Revenue Prediction Using Machine Learning Algorithms

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wtel-V6miQ8kzGv6IJmoUarg0HCspDtc
"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import ast
from wordcloud import WordCloud
import seaborn as sns
from scipy.stats import skew, boxcox
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import time
import plotly.offline as py
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split, KFold

import warnings
warnings.filterwarnings('ignore')
!pip install featuretools

# Downloading the csv file from your GitHub account

train_url = "https://raw.githubusercontent.com/Kilenk/inputs/main/train.csv"
test_url = "https://raw.githubusercontent.com/Kilenk/inputs/main/test.csv"
result_url = "https://raw.githubusercontent.com/Kilenk/inputs/main/sample_submission.csv"
# Reading the downloaded content and turning it into a pandas dataframe
train = pd.read_csv(train_url)
test = pd.read_csv(test_url)

print(train.shape)
print(test.shape)

# Revising some wrong information

train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning
train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          
train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs
train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven
train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout 
train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty
train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood
train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II
train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada
train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol
train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip
train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times
train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman
train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   
train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy 
train.loc[train['id'] == 1542,'budget'] = 1              # All at Once
train.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II
train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp
train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit
train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon
train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed
train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget
train.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers
train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus
train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams
train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D
train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture
test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal
test.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick
test.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise
test.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2
test.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II
test.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth
test.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values
test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family
test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage
test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee

# Lets see the Unique Value Distribution of Object features
Object_col = train.select_dtypes('object').columns
Analysis = []

for columns in Object_col:
    Unique = train[columns].nunique()
    Analysis.append([str(columns),Unique])

Analysis = np.array(Analysis)
plt.figure(figsize = (10,10))
plt.bar(x = Analysis[:,0],height = Analysis[:,1],width = 0.5,color = "green",edgecolor = "black")
plt.gca().invert_xaxis()
plt.xlabel("Features")
plt.ylabel("Unique Count")
plt.title("Distribution of Object Column")
plt.xticks(rotation = 70)
print("Total Size of dataset: {}".format(len(train)))

train.describe()

test.info()

# Lets see the Unique Value Distribution of Object features
Object_col = test.select_dtypes('object').columns
Analysis = []

for columns in Object_col:
    Unique = test[columns].nunique()
    Analysis.append([str(columns),Unique])

Analysis = np.array(Analysis)
plt.figure(figsize = (10,10))
plt.bar(x = Analysis[:,0],height = Analysis[:,1],width = 0.5,color = "green",edgecolor = "black")
plt.gca().invert_xaxis()
plt.xlabel("Features")
plt.ylabel("Unique Count")
plt.title("Distribution of Object Column")
plt.xticks(rotation = 70)
print("Total Size of dataset: {}".format(len(test)))

test["revenue"] = np.nan

test.info()

data = train.append(test,ignore_index=True)

data.info()

data.describe()

# Lets see the Unique Value Distribution of Object features
Object_col = data.select_dtypes('object').columns
Analysis = []

for columns in Object_col:
    Unique = data[columns].nunique()
    Analysis.append([str(columns),Unique])

Analysis = np.array(Analysis)
plt.figure(figsize = (10,10))
plt.bar(x = Analysis[:,0],height = Analysis[:,1],width = 0.6,color = "red",edgecolor = "black")
plt.gca().invert_xaxis()
plt.xlabel("Features")
plt.ylabel("Unique Count")
plt.title("Distribution of Object Column")
plt.xticks(rotation = 50)
print("Total Size of dataset: {}".format(len(data)))

"""# data pre processing """

columns = data.select_dtypes('object').columns
print(columns)

# print the first 10 Ids and its objects 
def display(input):
  for id,text in enumerate(data.loc[:10,input]):
    print(id,text)

"""##### Movie titles"""

# print the first 10 Ids and its objects 
display('belongs_to_collection')

# Lets extract the movie names and make them as a feature 
Value = []
for instance in data.loc[:,'belongs_to_collection']:
    if isinstance(instance,str):  # Because nan in float instance and rest is string instance
        Value.append(ast.literal_eval(instance)[0]['name'])  # Extracting out the name
    else:
        Value.append(np.nan)

data['belongs_to_collection'] = Value
data['belongs_to_collection'].head(5)

"""##### Genres"""

# print the first 10 Ids and its Genres 
display('genres')

# Lets Extract out the name first
Value = []
Count = []
for instance in data.loc[:,'genres']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total Genres of One Movie
        for i in X:
            Value.append(str(i['name']))
    else:
        Count.append(0)
data['genres'] = Count

"""##### Homepage"""

display('homepage')

data['homepage'].value_counts()

data.drop(['homepage'],axis = 1,inplace = True)

"""##### IMDB_Id"""

display('imdb_id')

data['imdb_id'].value_counts()

print(np.any(np.array(data['imdb_id'].value_counts()) == 1))

data.drop(['imdb_id'],axis = 1,inplace = True)

"""##### Original Title"""

display('original_title')

data['original_title'].value_counts()

data.drop(['original_title'],axis = 1,inplace = True)

"""##### Overview"""

display('overview')

data.drop(['overview'],axis = 1,inplace = True)

"""##### Poster Path"""

display('poster_path')

data['poster_path'].value_counts()      # All Unique Value We can Remove this attribute.
print(np.any(np.array(data['poster_path'].value_counts()) == 1))
data.drop(['poster_path'],axis = 1,inplace = True)

"""##### Time"""

data['release_date'] = pd.to_datetime(data['release_date'],format="%m/%d/%y")

data['release_date'].head(10)

"""##### Production Companies"""

display('production_companies')

# Lets Extract out the name first
Value = []
Count = []
for instance in data.loc[:,'production_companies']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total Production Companirs of One Movie
        for i in X:
            Value.append(str(i['name']))
    else:
        Count.append(0)

pd.DataFrame(Value).value_counts().index[:5] # We have to work on it and bring in right format

Correct_index = []
for index in pd.DataFrame(Value).value_counts().index:
    Correct_index.append(index[0])
print(Correct_index[:5])

data['production_companies'] = Count

"""##### Production Countries"""

display('production_countries')

# Same treatment we can give it as above
# Lets Extract out the name first
Value = []
Count = []
for instance in data.loc[:,'production_countries']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total Production Countries of One Movie
        for i in X:
            Value.append(str(i['name']))
    else:
        Count.append(0)

Value = pd.DataFrame(Value).rename(columns = {0:"Name"})
df = Value.value_counts()

df.loc[df > 50].plot(kind = "bar",color = "blue",edgecolor = "black",figsize = (20,5)) 
plt.xticks(rotation = 50)
plt.xlabel("Production Countries")
plt.ylabel("Count of Total Movies")
plt.show()
data['production_countries'] = Count
# Plotting the the Countries who created more than 50 movie

"""##### Spoken Languages"""

display('spoken_languages')

# We can Do same treatment as Earlier
# Lets Extract out the name first
Value = []
Count = []
for instance in data.loc[:,'spoken_languages']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total Languages of One Movie
        for i in X:
            Value.append(str(i['name']))
    else:
        Count.append(0)

Value[:10]

Correct_index = []
for index in pd.DataFrame(Value).value_counts().index:
    Correct_index.append(index[0])
print(Correct_index[:5])

data['spoken_languages'] = Count

"""##### Tagline"""

display('tagline')

data.drop(['tagline'],inplace = True,axis = 1)

"""##### Title"""

display('title')

data['title'].value_counts()

data.drop(['title'],inplace = True,axis = 1)

"""##### Keywords"""

display('Keywords')

# We can Do same treatment as Earlier

Count = []
for instance in data.loc[:,'Keywords']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total Keywords of One Movie
    else:
        Count.append(0)

data['Keywords'] = Count

"""##### cast"""

display('cast')

# We can Do same treatment as Earlier

Count = []
for instance in data.loc[:,'cast']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total casts of One Movie
    else:
        Count.append(0)

data['cast'] = Count

"""crew"""

display('crew')

# We can Do same treatment as Earlier

Count = []
for instance in data.loc[:,'crew']:
    if isinstance(instance,str):
        X = ast.literal_eval(instance)
        Count.append(len(X))   # Count Keep tracks of total crew of One Movie
    else:
        Count.append(0)

data['crew'] = Count

data.describe()

data.info()

data.head(1)

# Lets see Null values Now.
Null_Values = pd.DataFrame(data.isnull().sum()).rename(columns = {0:'Total'})
Null_Values['Percent'] = Null_Values['Total']/len(data)
Null_Values.sort_values('Percent',ascending=False).head()

# Lets Manually first store the index where Null values are present becasue Label Encoding will also encode Null values
index = list(Null_Values[Null_Values['Total'] >= 1].index)
index.remove('revenue')
Position = []
for col in index:
    temp = data[data[col].isnull()].index
    Position.append([col,temp])

from sklearn.preprocessing import LabelEncoder
# Lets Apply Label Encoding to object Columns
Object_Columns = data.select_dtypes('object').columns
data[Object_Columns] = data.select_dtypes('object').apply(LabelEncoder().fit_transform)

data.head(1)

for instance in Position:
    data.loc[instance[1],instance[0]] = np.nan

data.head(1)

# Lets see how many Unique Values are there in Integer columns
Integer_Columns = [col for col in data if 'int' in str(data[col].dtypes)]
data[Integer_Columns].nunique().value_counts().sort_index().plot(kind = 'bar',figsize = (8,8),edgecolor = 'blue',color = 'red')
plt.xlabel('Unique Count if the Feature')
plt.ylabel('Number of Feature')
plt.show()

import warnings
warnings.filterwarnings('ignore',category = FutureWarning)

# Lets See the Distribution of Each of them
import seaborn as sns
plt.figure(figsize=(20,20))
color = ['red','green','blue','orange','pink','yellow']
for idx,col in enumerate(Integer_Columns):
    ax = plt.subplot(4,3,idx+1)
    color_idx = np.random.randint(0,len(color))
    sns.violinplot(data[col],ax=ax,color=color[color_idx],linewidth=2)

# Lets see how they are related to our target Variable
import warnings
warnings.filterwarnings('ignore',category = FutureWarning)

# Lets See the Distribution of Each of them
import seaborn as sns
plt.figure(figsize=(20,20))
color = ['red','green','blue','orange','pink','yellow']
for idx,col in enumerate(Integer_Columns):
    ax = plt.subplot(4,3,idx+1)
    color_idx = np.random.randint(0,len(color))
    sns.scatterplot(data[col],data['revenue'],ax=ax,color=color[color_idx],linewidth=2)

    
# No Clear Correlation are Visible. Budget and revenue seems to be correlated

import warnings
warnings.filterwarnings('ignore',category = FutureWarning)

Float_columns = [col for col in data if 'float' in str(data[col].dtypes)]
Float_columns.remove('revenue')
# Lets See the Distribution of Float columns Each of them
import seaborn as sns
plt.figure(figsize=(20,20))
color = ['red','green','blue','orange','pink','yellow']
for idx,col in enumerate(Float_columns):
    ax = plt.subplot(4,3,idx+1)
    color_idx = np.random.randint(0,len(color))
    Mean,Median = np.mean(data[col]),np.median(data[col])
    sns.kdeplot(data[col],ax=ax,color=color[color_idx],linewidth=2)

    
# Some of them are Skewed. Some of them are almost gaussian centered at their respective mean

# Lets see how they are related to our target Variable
import warnings
warnings.filterwarnings('ignore',category = FutureWarning)

# Lets See the Distribution of Each of them
import seaborn as sns
plt.figure(figsize=(20,20))
color = ['red','green','blue','orange','pink','yellow']
for idx,col in enumerate(Float_columns):
    ax = plt.subplot(4,3,idx+1)
    color_idx = np.random.randint(0,len(color))
    sns.scatterplot(data[col],data['revenue'],ax=ax,color=color[color_idx],linewidth=2)

    
# No Clear Correlation are Visible. Budget and revenue seems to be correlated

# Lets see Null values Again.
Null_Values = pd.DataFrame(data.isnull().sum()).rename(columns = {0:'Total'})
Null_Values['Percent'] = Null_Values['Total']/len(data)
Null_Values.sort_values('Percent',ascending=False).head(10)

# Lets see what values we should Put in these
Null_Columns = list(Null_Values[Null_Values['Total'] >= 1].index)
Null_Columns.remove('revenue')
import warnings
warnings.filterwarnings('ignore',category = FutureWarning)

# Lets See the Distribution of Each of them
import seaborn as sns
plt.figure(figsize=(20,20))
color = ['red','green','blue','orange','pink','yellow']
for idx,col in enumerate(Null_Columns):
    ax = plt.subplot(3,3,idx+1)
    color_idx = np.random.randint(0,len(color))
    sns.histplot(data[col],ax=ax,color=color[color_idx],linewidth=2)

# Lets fill them with the median Value

data.head(5)

# Lets Start Feature Engineering
# Lets first remove any redundant feature having correlation greater than 95%
Correlation_Matrix = data.corr()
Triu = Correlation_Matrix.where(np.triu(np.ones(Correlation_Matrix.shape),k = 1).astype(np.bool))  # Will return Upper triangle
Redundant_col = [col for col in Triu if np.any(Triu[col].abs() >= 0.95)]

data.drop(Redundant_col,axis = 1,inplace = True)

data.info()

data.head(1)

"""# Featuere_tools"""

from sklearn.preprocessing import StandardScaler
OriginalLabel = data.loc[data['revenue'].notnull(),'revenue']
Columns = data.columns

# Lets Manually Create Some Features
import featuretools as ft
es = ft.EntitySet(id = 'TMDB')
es = es.add_dataframe(dataframe_name="data_tmdb",dataframe=data,make_index=True,index = 'TMDB_id')

es

es['data_tmdb']

feature_matrix,feature_dfs = ft.dfs(entityset=es, target_dataframe_name="data_tmdb",trans_primitives=["add_numeric","cum_mean","cum_sum","percentile"],max_depth=2)

feature_matrix.shape

Label = feature_matrix["revenue"]
Remove_col = []
for col in feature_matrix:
    if "revenue" in str(col):
        Remove_col.append(col)
Remove_col[:5]

feature_matrix.drop(Remove_col,axis = 1,inplace = True)

feature_matrix.head()

Correlation_Matrix = feature_matrix.corr()
Triu = Correlation_Matrix.where(np.triu(np.ones(Correlation_Matrix.shape),k = 1).astype(np.bool))  # Will return Upper triangle
Redundant_col = [col for col in Triu if np.any(Triu[col].abs() >= 0.95)]

feature_matrix.drop(Redundant_col,axis = 1,inplace = True)

feature_matrix.head()

feature_matrix['revenue'] = Label
del Label

Train = feature_matrix.loc[feature_matrix['revenue'].notnull(),:]
Test = feature_matrix.loc[feature_matrix['revenue'].isnull(),:]

Label = OriginalLabel
Train.drop(['revenue'],inplace = True,axis = 1)
Test.drop(['revenue'],inplace = True,axis = 1)

Features = Train.columns

len(Features)

Label = Label.values

len(Label)

"""# Random Forest Regressor"""

# Lets start Machine Learning From Here
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score,make_scorer
#Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).
#A constant model that always predicts the expected value of y, disregarding the input features,
#would get a R^2 score of 0.0.

imputer = SimpleImputer(strategy="median")
score_fn = make_scorer(r2_score,greater_is_better=True)

Train = imputer.fit_transform(Train)
Test = imputer.transform(Test)

from sklearn.model_selection import cross_val_score

from sklearn.ensemble import RandomForestRegressor   # Our Base Model 
from sklearn.model_selection import cross_val_score

RandomForest = RandomForestRegressor(n_estimators=100,n_jobs=-1,random_state=42)
cv_score = cross_val_score(RandomForest,Train,Label,cv = 10,scoring=score_fn)

print("10 Fold Cross Validation Mean R2_SCORE: {0} with Deviation: {1}".format(round(np.mean(cv_score),3),round(cv_score.std(),3)))

RandomForest.fit(Train,Label)
feature_importance = pd.DataFrame({'Features':Features,'Importance':RandomForest.feature_importances_})

def Plot_Importance(df,count = 5,threshold = 0.95):
    df['Importance'] = df['Importance']/df['Importance'].sum()
    df = df.sort_values('Importance',ascending = False).reset_index(drop = True)
    
    df['cum_sum'] = np.cumsum(df['Importance'])
    
    # Plotting Values
    df.loc[:count,:].plot(kind = 'barh',x = 'Features',y = 'Importance',color = 'green',
                          edgecolor = 'blue',figsize = (5,5),linewidth = 2)
    plt.xlabel("Normalized Importance")
    plt.xscale("log")
    plt.gca().invert_yaxis()
    
    plt.show()
    
    if threshold:
        min_ = np.min(np.where(df['cum_sum'] > threshold))
        plt.xlabel('# Features', size = 10)
        plt.ylabel('Cumulative Importance', size = 10)
        plt.title('Cumulative Importance of Features', size = 10)
        plt.plot(np.arange(len(df)),df['cum_sum'],color = 'red')
        plt.vlines(min_ + 1,ymin = 0,ymax = 1,color = 'black',linestyles='dotted')
        plt.show()
        print('Number of Columns required for {0} threshold is: {1}'.format(threshold,min_+1))
    return df

feature_importance = Plot_Importance(feature_importance,count=10)

"""# XGBoost"""

# Reading the downloaded content and turning it into a pandas dataframe
train = pd.read_csv(train_url)
test = pd.read_csv(test_url)

## From this function, you can convert release_date column from the character data type to the datetime data type

def date_features(df):
    df['release_date'] = pd.to_datetime(df['release_date'])
    df['release_year'] = df['release_date'].dt.year
    df['release_month'] = df['release_date'].dt.month
    df['release_day'] = df['release_date'].dt.day
    df['release_quarter'] = df['release_date'].dt.quarter
    df.drop(columns=['release_date'], inplace=True)
    return df

train=date_features(train)
test=date_features(test)

train['release_year'].head(10)

train['release_year'].iloc[np.where(train['release_year']> 2022)][:10]

train['release_year']=np.where(train['release_year']> 2019, train['release_year']-100, train['release_year'])
test['release_year']=np.where(test['release_year']> 2019, test['release_year']-100, test['release_year'])

## Filling NA values with mode of each column

fillna_column = {'release_year':'mode','release_month':'mode',
                'release_day':'mode'}

for k,v in fillna_column.items():
    if v == 'mode':
        fill = train[k].mode()[0]
    else:
        fill = v
    print(k, ': ', fill)
    train[k].fillna(value = fill, inplace = True)
    test[k].fillna(value = fill, inplace = True)

# Putting revised year, month, and day together 

def year_month_together(df):
    year = df["release_year"].astype(int).copy().astype(str)
    month=df['release_month'].astype(int).copy().astype(str)
    day=df['release_day'].astype(int).copy().astype(str) 
    df["release_date"]=  month.str.cat(day.str.cat(year,sep="/"), sep ="/") 
    df['release_date']=pd.to_datetime(df['release_date'],format="%m/%d/%Y")
    df['release_dow'] = df['release_date'].dt.dayofweek
    return df 

train=year_month_together(train)
test=year_month_together(test)

train['release_date'].head(10)

# Creating correlation matrix 

corr = train.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(220, 10, as_cmap=True)

sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

dict_columns = ['belongs_to_collection', 'genres', 'production_companies',
                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']

def text_to_dict(df):
    for column in dict_columns:
        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )
    return df
        
train = text_to_dict(train)
test = text_to_dict(test)

"""## **Belong_to collection**"""

# Counting NAs as 0
train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()

train['collection_name'] = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)
train['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)

test['collection_name'] = test['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)
test['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)

train = train.drop(['belongs_to_collection'], axis=1)
test = test.drop(['belongs_to_collection'], axis=1)

# Most common collection 
train['collection_name'].value_counts()[1:10]

"""## **Genres**"""

from collections import Counter

train['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()
list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)
most_common_genres=Counter([i for j in list_of_genres for i in j]).most_common()
fig = plt.figure(figsize=(10, 6))
data=dict(most_common_genres)
names = list(data.keys())
values = list(data.values())

plt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='teal')
plt.xlabel('Count')
plt.title('Movie Genre Count')
plt.show()

train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)
train['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]
for g in top_genres:
    train['genre_' + g] = train['all_genres'].apply(lambda x: 1 if g in x else 0)
    
test['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)
test['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_genres:
    test['genre_' + g] = test['all_genres'].apply(lambda x: 1 if g in x else 0)

train = train.drop(['genres'], axis=1)
test = test.drop(['genres'], axis=1)

import nltk
nltk.download("stopwords") 
from nltk.corpus import stopwords

# Movie title text analysis 

text = " ".join(review for review in train.title)
print ("There are {} words in the combination of all review.".format(len(text)))

stopwords = set(stopwords.words('english'))
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

fig = plt.figure(figsize=(11, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('movie title in train data')
plt.axis("off")
plt.show()

# Text analysis on top 4 movie genres title 

drama=train.loc[train['genre_Drama']==1,]
comedy=train.loc[train['genre_Comedy']==1,]
action=train.loc[train['genre_Action']==1,]
thriller=train.loc[train['genre_Thriller']==1,]



text_drama = " ".join(review for review in drama.title)
text_comedy = " ".join(review for review in comedy.title)
text_action = " ".join(review for review in action.title)
text_thriller = " ".join(review for review in thriller.title)


wordcloud1 = WordCloud(stopwords=stopwords, background_color="white",colormap="Reds").generate(text_drama)
wordcloud2 = WordCloud(stopwords=stopwords, background_color="white",colormap="Blues").generate(text_comedy)
wordcloud3 = WordCloud(stopwords=stopwords, background_color="white",colormap="Greens").generate(text_action)
wordcloud4 = WordCloud(stopwords=stopwords, background_color="white",colormap="Greys").generate(text_thriller)


fig = plt.figure(figsize=(20, 10))

plt.subplot(221)
plt.imshow(wordcloud1, interpolation='bilinear')
plt.title('drama movie title')
plt.axis("off")

plt.subplot(222)
plt.imshow(wordcloud2, interpolation='bilinear')
plt.title('comedy movie title')
plt.axis("off")
plt.show()

fig = plt.figure(figsize=(20, 10))

plt.subplot(223)
plt.imshow(wordcloud3, interpolation='bilinear')
plt.title('action movie title')
plt.axis("off")

plt.subplot(224)
plt.imshow(wordcloud4, interpolation='bilinear')
plt.title('thriller movie title')
plt.axis("off")
plt.show()

drama_revenue=drama.groupby(['release_year']).mean()['revenue']
comedy_revenue=comedy.groupby(['release_year']).mean()['revenue']
action_revenue=action_revenue=action.groupby(['release_year']).mean()['revenue']
thriller_revenue=thriller.groupby(['release_year']).mean()['revenue']

revenue_concat = pd.concat([drama_revenue,comedy_revenue,action_revenue,thriller_revenue], axis=1)
revenue_concat.columns=['drama','comedy','action','thriller']
revenue_concat.index=train.groupby(['release_year']).mean().index

# Mean revenue over years by top 4 genres 
import plotly.graph_objs as go
import plotly.offline as py
data = [go.Scatter(x=revenue_concat.index, y=revenue_concat.drama, name='drama'), go.Scatter(x=revenue_concat.index, y=revenue_concat.comedy, name='comedy'),
       go.Scatter(x=revenue_concat.index, y=revenue_concat.action, name='action'),go.Scatter(x=revenue_concat.index, y=revenue_concat.thriller, name='thriller')]
layout = go.Layout(dict(title = 'Mean Revenue by Top 4 Movie Genres Over Years',
                  xaxis = dict(title = 'Year'),
                  yaxis = dict(title = 'Revenue'),
                  ),legend=dict(
                orientation="v"))
py.iplot(dict(data=data, layout=layout))

"""## **Production Companies**"""

# Counting the frequency of production company 
list_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)

most_common_companies=Counter([i for j in list_of_companies for i in j]).most_common(20)

fig = plt.figure(figsize=(10, 6))
data=dict(most_common_companies)
names = list(data.keys())
values = list(data.values())

plt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='brown')
plt.xlabel('Count')
plt.title('Top 20 Production Company Count')
plt.show()

train['num_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)
train['all_production_companies'] = train['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]
for g in top_companies:
    train['production_company_' + g] = train['all_production_companies'].apply(lambda x: 1 if g in x else 0)
    
test['num_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)
test['all_production_companies'] = test['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_companies:
    test['production_company_' + g] = test['all_production_companies'].apply(lambda x: 1 if g in x else 0)

train = train.drop(['production_companies', 'all_production_companies'], axis=1)
test = test.drop(['production_companies', 'all_production_companies'], axis=1)



"""## **Production Countries**"""

list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)
most_common_countries=Counter([i for j in list_of_countries for i in j]).most_common(20)

fig = plt.figure(figsize=(10, 6))
data=dict(most_common_countries)
names = list(data.keys())
values = list(data.values())

plt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='purple')
plt.xlabel('Count')
plt.title('Country Count')
plt.show()

train['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)
train['all_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]
for g in top_countries:
    train['production_country_' + g] = train['all_countries'].apply(lambda x: 1 if g in x else 0)
    
test['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)
test['all_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_countries:
    test['production_country_' + g] = test['all_countries'].apply(lambda x: 1 if g in x else 0)

train = train.drop(['production_countries', 'all_countries'], axis=1)
test = test.drop(['production_countries', 'all_countries'], axis=1)

# English is the majority spoken language 
list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)

most_common_languages=Counter([i for j in list_of_languages for i in j]).most_common(20)

fig = plt.figure(figsize=(10, 6))
data=dict(most_common_languages)
names = list(data.keys())
values = list(data.values())

plt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='gray')
plt.xlabel('Count')
plt.title('Language Count')
plt.show()

train['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)
train['all_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')
top_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]
for g in top_languages:
    train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)
    
test['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)
test['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')
for g in top_languages:
    test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)

train = train.drop(['spoken_languages', 'all_languages'], axis=1)
test = test.drop(['spoken_languages', 'all_languages'], axis=1)

"""## **Keywords**"""

list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)

most_common_keywords=Counter([i for j in list_of_keywords for i in j]).most_common(20)

fig = plt.figure(figsize=(10, 6))
data=dict(most_common_keywords)
names = list(data.keys())
values = list(data.values())

plt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='purple')
plt.xlabel('Count')
plt.title('Top 20 Most Common Keyword Count')
plt.show()

# Text analysis on keywords by top 4 genres


text_drama = " ".join(review for review in drama['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))
text_comedy = " ".join(review for review in comedy['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))
text_action = " ".join(review for review in action['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))
text_thriller = " ".join(review for review in thriller['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))


wordcloud1 = WordCloud(stopwords=stopwords, background_color="white",colormap="Reds").generate(text_drama)
wordcloud2 = WordCloud(stopwords=stopwords, background_color="white",colormap="Blues").generate(text_comedy)
wordcloud3 = WordCloud(stopwords=stopwords, background_color="white",colormap="Greens").generate(text_action)
wordcloud4 = WordCloud(stopwords=stopwords, background_color="white",colormap="Greys").generate(text_thriller)


fig = plt.figure(figsize=(25, 20))

plt.subplot(221)
plt.imshow(wordcloud1, interpolation='bilinear')
plt.title('Drama Keywords')
plt.axis("off")

plt.subplot(222)
plt.imshow(wordcloud2, interpolation='bilinear')
plt.title('Comedy Keywords')
plt.axis("off")
plt.show()

fig = plt.figure(figsize=(25, 20))

plt.subplot(223)
plt.imshow(wordcloud3, interpolation='bilinear')
plt.title('Action Keywords')
plt.axis("off")

plt.subplot(224)
plt.imshow(wordcloud4, interpolation='bilinear')
plt.title('Thriller Keywords')
plt.axis("off")
plt.show()

train['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)
train['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]
for g in top_keywords:
    train['keyword_' + g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)
    
test['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)
test['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_keywords:
    test['keyword_' + g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)

train = train.drop(['Keywords', 'all_Keywords'], axis=1)
test = test.drop(['Keywords', 'all_Keywords'], axis=1)

"""## **Cast**"""

list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)
train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)
train['all_cast'] = train['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(30)]
for g in top_cast_names:
    train['cast_name_' + g] = train['all_cast'].apply(lambda x: 1 if g in x else 0)

    
test['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)
test['all_cast'] = test['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_cast_names:
    test['cast_name_' + g] = test['all_cast'].apply(lambda x: 1 if g in x else 0)

#Mean revenue comparison of 10 most common actor/actress 

cast_name_Samuel_L_Jackson=train.loc[train['cast_name_Samuel L. Jackson']==1,]
cast_name_Robert_De_Niro=train.loc[train['cast_name_Robert De Niro']==1,]
cast_name_Morgan_Freeman=train.loc[train['cast_name_Morgan Freeman']==1,]
cast_name_J_K_Simmons=train.loc[train['cast_name_J.K. Simmons']==1,]
cast_name_Bruce_Willis=train.loc[train['cast_name_Bruce Willis']==1,]
cast_name_Liam_Neeson=train.loc[train['cast_name_Liam Neeson']==1,]
cast_name_Susan_Sarandon=train.loc[train['cast_name_Susan Sarandon']==1,]
cast_name_Bruce_McGill=train.loc[train['cast_name_Bruce McGill']==1,]
cast_name_John_Turturro=train.loc[train['cast_name_John Turturro']==1,]
cast_name_Forest_Whitaker=train.loc[train['cast_name_Forest Whitaker']==1,]


cast_name_Samuel_L_Jackson_revenue=cast_name_Samuel_L_Jackson.mean()['revenue']
cast_name_Robert_De_Niro_revenue=cast_name_Robert_De_Niro.mean()['revenue']
cast_name_Morgan_Freeman_revenue=cast_name_Morgan_Freeman.mean()['revenue']
cast_name_J_K_Simmons_revenue=cast_name_J_K_Simmons.mean()['revenue']
cast_name_Bruce_Willis_revenue=cast_name_Bruce_Willis.mean()['revenue']
cast_name_Liam_Neeson_revenue=cast_name_Liam_Neeson.mean()['revenue']
cast_name_Susan_Sarandon_revenue=cast_name_Susan_Sarandon.mean()['revenue']
cast_name_Bruce_McGill_revenue=cast_name_Bruce_McGill.mean()['revenue']
cast_name_John_Turturro_revenue=cast_name_John_Turturro.mean()['revenue']
cast_name_Forest_Whitaker_revenue=cast_name_Forest_Whitaker.mean()['revenue']


cast_revenue_concat = pd.Series([cast_name_Samuel_L_Jackson_revenue,cast_name_Robert_De_Niro_revenue,cast_name_Morgan_Freeman_revenue,cast_name_J_K_Simmons_revenue,
                                cast_name_Bruce_Willis_revenue,cast_name_Liam_Neeson_revenue,cast_name_Susan_Sarandon_revenue,cast_name_Bruce_McGill_revenue,
                                cast_name_John_Turturro_revenue,cast_name_Forest_Whitaker_revenue])
cast_revenue_concat.index=['Samuel L. Jackson','Robert De Niro','Morgan Freeman','J.K. Simmons','Bruce Willis','Liam Neeson','Susan Sarandon','Bruce McGill',
                            'John Turturro','Forest Whitaker']


fig = plt.figure(figsize=(13, 7))
cast_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='Mean Revenue (100 million dollars) by Top 10 Most Common Cast')
plt.xlabel('Revenue (100 million dollars)')

# Consider other factors like gender and characters 

list_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)
list_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)


train['genders_0'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))
train['genders_1'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))
train['genders_2'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))
top_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]
for g in top_cast_characters:
    train['cast_character_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)
    

test['genders_0'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))
test['genders_1'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))
test['genders_2'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))
for g in top_cast_characters:
    test['cast_character_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)

train = train.drop(['cast'], axis=1)
test = test.drop(['cast'], axis=1)

"""## **Crew**"""

list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)
train['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)
train['all_crew'] = train['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
top_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(30)]
for g in top_crew_names:
    train['crew_name_' + g] = train['all_crew'].apply(lambda x: 1 if g in x else 0)

test['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)
test['all_crew'] = test['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')
for g in top_crew_names:
    test['crew_name_' + g] = test['all_crew'].apply(lambda x: 1 if g in x else 0)

crew_name_Avy_Kaufman=train.loc[train['crew_name_Avy Kaufman']==1,]
crew_name_Robert_Rodriguez=train.loc[train['crew_name_Robert Rodriguez']==1,]
crew_name_Deborah_Aquila=train.loc[train['crew_name_Deborah Aquila']==1,]
crew_name_James_Newton_Howard=train.loc[train['crew_name_James Newton Howard']==1,]
crew_name_Mary_Vernieu=train.loc[train['crew_name_Mary Vernieu']==1,]
crew_name_Steven_Spielberg=train.loc[train['crew_name_Steven Spielberg']==1,]
crew_name_Luc_Besson=train.loc[train['crew_name_Luc Besson']==1,]
crew_name_Jerry_Goldsmith=train.loc[train['crew_name_Jerry Goldsmith']==1,]
crew_name_Francine_Maisler=train.loc[train['crew_name_Francine Maisler']==1,]
crew_name_Tricia_Wood=train.loc[train['crew_name_Tricia Wood']==1,]


crew_name_Avy_Kaufman_revenue=crew_name_Avy_Kaufman.mean()['revenue']
crew_name_Robert_Rodriguez_revenue=crew_name_Robert_Rodriguez.mean()['revenue']
crew_name_Deborah_Aquila_revenue=crew_name_Deborah_Aquila.mean()['revenue']
crew_name_James_Newton_Howard_revenue=crew_name_James_Newton_Howard.mean()['revenue']
crew_name_Mary_Vernieu_revenue=crew_name_Mary_Vernieu.mean()['revenue']
crew_name_Steven_Spielberg_revenue=crew_name_Steven_Spielberg.mean()['revenue']
crew_name_Luc_Besson_revenue=crew_name_Luc_Besson.mean()['revenue']
crew_name_Jerry_Goldsmith_revenue=crew_name_Jerry_Goldsmith.mean()['revenue']
crew_name_Francine_Maisler_revenue=crew_name_Francine_Maisler.mean()['revenue']
crew_name_Tricia_Wood_revenue=crew_name_Tricia_Wood.mean()['revenue']


crew_revenue_concat = pd.Series([crew_name_Avy_Kaufman_revenue,crew_name_Robert_Rodriguez_revenue,crew_name_Deborah_Aquila_revenue,crew_name_James_Newton_Howard_revenue,
                                crew_name_Mary_Vernieu_revenue,crew_name_Steven_Spielberg_revenue,crew_name_Luc_Besson_revenue,crew_name_Jerry_Goldsmith_revenue,
                                crew_name_Francine_Maisler_revenue,crew_name_Tricia_Wood_revenue])
crew_revenue_concat.index=['Avy Kaufman','Robert Rodriguez','Deborah Aquila','James Newton Howard','Mary Vernieu','Steven Spielberg','Luc Besson','Jerry Goldsmith',
                            'Francine Maisler','Tricia Wood']


fig = plt.figure(figsize=(13, 7))
crew_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='Mean Revenue (100 million dollars) by Top 10 Most Common Crew')
plt.xlabel('Revenue (100 million dollars)')

# Consider other factors like crew jobs, gender, and department 

list_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)
list_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)
list_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)


train['genders_0'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))
train['genders_1'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))
train['genders_2'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))
top_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]
for g in top_cast_characters:
    train['crew_character_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)
top_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]
for j in top_crew_jobs:
    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))
top_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]
for j in top_crew_departments:
    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) 
    

    
test['genders_0'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))
test['genders_1'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))
test['genders_2'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))
for g in top_cast_characters:
    test['crew_character_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)
for j in top_crew_jobs:
    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))
for j in top_crew_departments:
    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) 

train = train.drop(['crew'], axis=1)
test = test.drop(['crew'], axis=1)

"""# **Feature Engineering**"""

# Plot the distribution of the revenue

fig = plt.figure(figsize=(30, 25))

plt.subplot(221)
train['revenue'].plot(kind='hist',bins=100)
plt.title('Distribution of Revenue')
plt.xlabel('Revenue')

plt.subplot(222)
np.log1p(train['revenue']).plot(kind='hist',bins=100)
plt.title('Train Log Revenue Distribution')
plt.xlabel('Log Revenue')


print('Skew of revenue attribute: %0.1f' % skew(train['revenue']))

# Adjusting other skewed variables such as popularity and budget

print('Skew of train budget attribute: %0.1f' % skew(train['budget']))
print('Skew of test budget attribute: %0.1f' % skew(test['budget']))
print('Skew of train popularity attribute: %0.1f' % skew(train['popularity']))
print('Skew of test popularity attribute: %0.1f' % skew(test['popularity']))

# Before log transformation and after log transformation for train budget and train popularity 
fig = plt.figure(figsize=(30, 25))

plt.subplot(221)
train['budget'].plot(kind='hist',bins=100)
plt.title('Train Budget Distribution')
plt.xlabel('Budget')

plt.subplot(222)
np.log1p(train['budget']).plot(kind='hist',bins=100)
plt.title('Train Log Budget Distribution')
plt.xlabel('Log Budget')

plt.show()

fig = plt.figure(figsize=(30, 25))

plt.subplot(223)
test['budget'].plot(kind='hist',bins=100)
plt.title('Train Popularity Distribution')
plt.xlabel('Popularity')

plt.subplot(224)
np.log1p(test['budget']).plot(kind='hist',bins=100)
plt.title('Train Log Popularity Distribution')
plt.xlabel('Log Popularity')
plt.show()

# Revising budget variable 

power_six = train.id[train.budget > 1000][train.revenue < 100]

for k in power_six :
    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000

# Putting log variables for skewed data 
train['log_budget']=np.log1p(train['budget'])
test['log_budget']=np.log1p(test['budget'])
train['log_popularity']=np.log1p(train['popularity'])
test['log_popularity']=np.log1p(test['popularity'])

def prepare(df):
    df['_budget_runtime_ratio'] = df['budget']/df['runtime'] 
    df['_budget_popularity_ratio'] = df['budget']/df['popularity']
    df['_budget_year_ratio'] = df['budget']/(df['release_year']*df['release_year'])
    df['_releaseYear_popularity_ratio'] = df['release_year']/df['popularity']
    df['_releaseYear_popularity_ratio2'] = df['popularity']/df['release_year']
    df['_year_to_log_budget'] = df['release_year'] / df['log_budget']
    df['_year_to_log_popularity'] = df['release_year'] / df['log_popularity']

    df['has_homepage'] = 0
    df.loc[pd.isnull(df['homepage']) ,"has_homepage"] = 1
    
    df['isTaglineNA'] = 0
    df.loc[df['tagline'] == 0 ,"isTaglineNA"] = 1 
    
    df['isTitleDifferent'] = 1
    df.loc[ df['original_title'] == df['title'] ,"isTitleDifferent"] = 0 

    df['isMovieReleased'] = 1
    df.loc[ df['status'] != "Released" ,"isMovieReleased"] = 0 

    df['original_title_letter_count'] = df['original_title'].str.len() 
    df['original_title_word_count'] = df['original_title'].str.split().str.len() 
    df['title_word_count'] = df['title'].str.split().str.len()
    df['overview_word_count'] = df['overview'].str.split().str.len()
    df['tagline_word_count'] = df['tagline'].str.split().str.len()
    df['meanruntimeByYear'] = df.groupby("release_year")["runtime"].aggregate('mean')
    df['meanPopularityByYear'] = df.groupby("release_year")["popularity"].aggregate('mean')
    df['meanBudgetByYear'] = df.groupby("release_year")["budget"].aggregate('mean')

    return df

train_new=prepare(train)
test_new=prepare(test)

train_new.to_csv("train_new.csv", index=False)
test_new.to_csv("test_new.csv", index=False)

drop_columns=['homepage','imdb_id','poster_path','status','title', 'release_date','tagline', 'overview', 'original_title','all_genres','all_cast',
             'original_language','collection_name','all_crew']
train_new=train_new.drop(drop_columns,axis=1)
test_new=test_new.drop(drop_columns,axis=1)

train_new.head(1)

print( "updated train dataset:", train_new.shape,"\n","updated test dataset: ",test_new.shape)

# Just double checking the difference of variables between train and test 
print(train_new.columns.difference(test_new.columns)) # good to go!

# Formating for modeling
from sklearn.model_selection import train_test_split
X = train_new.drop(['id','revenue'], axis=1)
y = np.log1p(train_new['revenue'])
X_test = test_new.drop(['id'], axis=1)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)

params = {'num_leaves': 30,
         'min_data_in_leaf': 20,
         'objective': 'regression',
         'max_depth': 5,
         'learning_rate': 0.01,
         "boosting": "gbdt",
         "feature_fraction": 0.9,
         "bagging_freq": 1,
         "bagging_fraction": 0.9,
         "bagging_seed": 11,
         "metric": 'rmse',
         "lambda_l1": 0.2,
         "verbosity": -1}

lgb_model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)
lgb_model.fit(X_train, y_train, 
        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',
        verbose=1000, early_stopping_rounds=200)

#eli5.show_weights(lgb_model, feature_filter=lambda x: x != '<BIAS>')

# Obtain from Random Search 

opt_parameters = {'random_state': 501, 'objective': 'regression', 'num_leaves': 40, 'min_data_in_leaf': 15, 'max_depth': 4, 'learning_rate': 0.01, 'boosting_type': 'gbdt'} 

params['learning_rate'] = opt_parameters['learning_rate']
params['max_depth'] = opt_parameters['max_depth']
params['num_leaves'] = opt_parameters['num_leaves']
params['min_data_in_leaf'] = opt_parameters['min_data_in_leaf']

n_fold = 5
random_seed=2222
folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=True, model=None):

    oof = np.zeros(X.shape[0])
    prediction = np.zeros(X_test.shape[0])
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        if model_type == 'sklearn':
            X_train, X_valid = X[train_index], X[valid_index]
        else:
            X_train, X_valid = X.values[train_index], X.values[valid_index]
        y_train, y_valid = y[train_index], y[valid_index]
        
        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)
            model.fit(X_train, y_train, 
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',
                    verbose=500, early_stopping_rounds=200)
            
            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)
            
        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=10000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)

        
        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)
        
        prediction += y_pred    
        
        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold
    
    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))
    
    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');
        
            return oof, prediction, feature_importance
        return oof, prediction
    
    else:
        return oof, prediction

import time
from sklearn.metrics import mean_squared_error

oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb')

xgb_params = {'eta': 0.01,
              'objective': 'reg:linear',
              'max_depth': 6,
              'min_child_weight': 3,
              'subsample': 0.8,
              'colsample_bytree': 0.8,
              'eval_metric': 'rmse',
              'seed': 11,
              'silent': True}

start = time.time()
oof_xgb, prediction_xgb = train_model(X, X_test, y, params=xgb_params, model_type='xgb')
end = time.time()
print("time elapsed:",end - start, "second")

sam_sub =pd.read_csv(result_url)

sam_sub['revenue'] = np.expm1(prediction_lgb)
sam_sub.to_csv("lgb.csv", index=False)
sam_sub['revenue'] = np.expm1(prediction_xgb)
sam_sub.to_csv("xgb.csv", index=False)
sam_sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) / 2)
sam_sub.to_csv("blend_lgb_xgb.csv", index=False)